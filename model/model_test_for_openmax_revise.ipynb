{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_data = pd.read_csv(\"hate_speech_binary_dataset.csv\", delimiter=\",\") # 혐오 문장\n",
    "genderbias_data = pd.read_csv('genderbias.csv', sep=',')  # 여성 비하 문장\n",
    "ilbe_data = pd.read_csv('badword.csv',encoding='CP949') # 일베 문장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문장 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 혐오 문장 처리\n",
    "hate_data.columns = ['comment', 'label'] # 컬럼 명 변경\n",
    "hate_data = hate_data.astype({'comment': 'str'})\n",
    "hate_data = hate_data[hate_data['label']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여성 비하 문장 처리\n",
    "del genderbias_data['bias']    #해당 데이터셋의 필요없는 열 제거\n",
    "del genderbias_data['hate']    #해당 데이터셋의 필요없는 열 제거\n",
    "genderbias_data['contain_gender_bias'] = genderbias_data['contain_gender_bias'].replace([False, True],[0,1])  # 구분하기 쉽게 기존의 표기를 0,1로 변경\n",
    "# genderbias_data = genderbias_data[['contain_gender_bias', 'comments']]    #구분하기 쉽게 열의 순서를 변경\n",
    "\n",
    "genderbias_data.columns = ['comment', 'label'] # 컬럼 명 변경\n",
    "genderbias_data = genderbias_data[genderbias_data['label']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일베 문장 처리\n",
    "ilbe_data = ilbe_data[['v2', 'v1']]    #구분하기 쉽게 열의 순서를 변경\n",
    "ilbe_data.columns = ['comment', 'label'] # 컬럼 명 변경\n",
    "ilbe_data = ilbe_data[ilbe_data['label']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hate_data : 100000\n",
      "genderbias_data : 1232\n",
      "ilbe_data : 2044\n"
     ]
    }
   ],
   "source": [
    "# 데이터 개수\n",
    "print(\"hate_data : %d\" % len(hate_data))\n",
    "print(\"genderbias_data : %d\" % len(genderbias_data))\n",
    "print(\"ilbe_data : %d\" % len(ilbe_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라벨링 및 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 렌덤하게 문장 추출 후 라벨링 거치는 함수\n",
    "def random_labeling(data, classified_data, label_num, str_num):  # label_num : 라벨링 시킬 값, str_num 추출할 문장 개수\n",
    "    random_data = classified_data.sample(n=str_num) # str_num 개의 행(문장) 랜덤 추출\n",
    "    random_data.loc[random_data.label == 1, 'label'] = label_num # label_num으로 값 변경\n",
    "    data = data.append(random_data)  # data 에 랜덤 추출된 데이터 추가\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>str_Declaration</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           comment  label\n",
       "0  str_Declaration     -1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_declaration = {\n",
    "    'comment' : [\"str_Declaration\"],\n",
    "    'label':[-1]\n",
    "}\n",
    "\n",
    "\n",
    "data = pd.DataFrame(data_declaration)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 개수를 모두 2000 개로 지정 각 \n",
    "# 라벨은 혐오:0, 여성비하: 1, 일베: 2로 지정됨\n",
    "data = random_labeling(data, hate_data, 0, 2000)\n",
    "data = random_labeling(data, genderbias_data, 1, 1200)\n",
    "data = random_labeling(data, ilbe_data, 2, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 첫번째 행 제거\n",
    "data = data.drop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 랜덤하게 섞기 \n",
    "data = data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>토론에서 지면 현피를 뜨는 지능 수준 잘 알았다 ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>요즘 신종사기가 이거임틀딱들이 40대 아줌매미 에게결혼하면 10억대 아파트 명의준다...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>아이들도 어른이 저도 재밌게 봤어요....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>어렸을때보고 지금다시봐도 재밌어요~ 캐스팅 자체가 고퀄리티</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>현아는 남친이 있어도 그 입술이 남친의 것이 아니라 팬들의 것임. 누구 맘대로 시술...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>사진땜시 기분 잣같아져서 ㅁㅈㅎ</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>와 씨발 중국이냐. 진짜 죠센징들은 날이 갈수록 퇴화하노.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>응 근데 ㅅㅂ 고졸 무스펙인데 따도 할수있으려나 모르겠다 다시 노가다나 해야하나</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>완전 재미었어요</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>백상에서도 인정 받은 드라마 나저씨를 왜 페미들은 인정안함?ㅋㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  label\n",
       "0           토론에서 지면 현피를 뜨는 지능 수준 잘 알았다 ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ      2\n",
       "1  요즘 신종사기가 이거임틀딱들이 40대 아줌매미 에게결혼하면 10억대 아파트 명의준다...      2\n",
       "2                            아이들도 어른이 저도 재밌게 봤어요....      0\n",
       "3                   어렸을때보고 지금다시봐도 재밌어요~ 캐스팅 자체가 고퀄리티      0\n",
       "4  현아는 남친이 있어도 그 입술이 남친의 것이 아니라 팬들의 것임. 누구 맘대로 시술...      1\n",
       "5                                  사진땜시 기분 잣같아져서 ㅁㅈㅎ      2\n",
       "6                  와 씨발 중국이냐. 진짜 죠센징들은 날이 갈수록 퇴화하노.       2\n",
       "7       응 근데 ㅅㅂ 고졸 무스펙인데 따도 할수있으려나 모르겠다 다시 노가다나 해야하나      2\n",
       "8                                           완전 재미었어요      0\n",
       "9               백상에서도 인정 받은 드라마 나저씨를 왜 페미들은 인정안함?ㅋㅋㅋ      1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 중복 및 null 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_overlap(data):\n",
    "    exist_overlap = len(data)  # 데이터 전체 개수\n",
    "    no_overlap = data['comment'].nunique()  # 중복 제거된 개수\n",
    "    if exist_overlap != no_overlap:\n",
    "        data.drop_duplicates(subset=['comment'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_overlap(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# null 값 확인\n",
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x_data, tokenizer):\n",
    "    tokenizer.fit_on_texts(x_data) # 데이터의 각 행별로 토큰화 수행\n",
    "    return tokenizer.texts_to_sequences(x_data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = data['comment']\n",
    "y_data = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "sequences = tokenize(x_data, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 희귀단어 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_rareword(tokenizer, threshold):\n",
    "    word_to_index = tokenizer.word_index \n",
    "    total_cnt = len(word_to_index) # 단어의 수\n",
    "    rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "    total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "    rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "    # 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "    for key, value in tokenizer.word_counts.items():\n",
    "        total_freq = total_freq + value\n",
    "\n",
    "        # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "        if(value < threshold):\n",
    "            rare_cnt = rare_cnt + 1\n",
    "            rare_freq = rare_freq + value\n",
    "\n",
    "    print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "    print(\"단어 집합(vocabulary)에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "    print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "등장 빈도가 1번 이하인 희귀 단어의 수: 23664\n",
      "단어 집합(vocabulary)에서 희귀 단어의 비율: 83.4561805678011\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 48.867320598864225\n"
     ]
    }
   ],
   "source": [
    "# 희귀 단어 확인\n",
    "detect_rareword(tokenizer, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련 데이터, 테스트 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 긴 길이\n",
    "def max_length(X_data):\n",
    "    return max(len(l) for l in X_data)\n",
    "\n",
    "# 훈련 학습 데이터 개수\n",
    "def num_dataset(raio, X_data):\n",
    "    num_train = int(len(X_data)*0.8)\n",
    "    num_test = int(len(X_data) - num_train)\n",
    "    return num_train, num_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 길이 계산\n",
    "X_data = sequences\n",
    "maxlen = max_length(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_data = np.array(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 0, ..., 2, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문장 길이 분포 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 최대 길이 : 194\n",
      "문장 평균 길이 : 9.323258\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEGCAYAAACdJRn3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAbvElEQVR4nO3dfZRddX3v8feHABE1ETAjKybABBu4AmqAgXKvYLFUCGAFbIXk1oJCG6GhQFFvk4uVVFduoYq6sNdguFCi5UFapOQKCIHLQ72GhwkEkvBQEhJkSG4ShUIAiSZ87x/7N2Rncs7M3jNnn3Mm+bzW2mv2+Z798J19TuabvX97/36KCMzMzMrYqdUJmJnZ8OPiYWZmpbl4mJlZaS4eZmZWmouHmZmVtnOrE6jKmDFjorOzs9VpmJkNK4sWLfplRHQMtNx2Wzw6Ozvp7u5udRpmZsOKpOeLLOfLVmZmVpqLh5mZlebiYWZmpbl4mJlZaS4eZmZWmouHmZmV5uJhZmaluXiYmVlpLh5mZlbadvuEeRU6Z9xWM77q0pOanImZWWv5zMPMzEpz8TAzs9JcPMzMrDQXDzMzK83Fw8zMSnPxMDOz0lw8zMysNBcPMzMrzcXDzMxKq6x4SLpG0jpJS3OxH0lanKZVkhaneKekX+feuzK3zmGSlkhaLukKSaoqZzMzK6bK7kmuBf4B+EFvICJO752XdDnwSm75FRExqcZ25gDTgAeB24HJwB2NT9fMzIqq7MwjIh4AXqr1Xjp7OA24ob9tSBoLjI6IhRERZIXolAanamZmJbWqzeNoYG1EPJuLTZD0mKT7JR2dYuOAntwyPSlWk6Rpkrolda9fv77xWZuZGdC64jGVrc861gD7RMQhwEXA9ZJGA7XaN6LeRiNibkR0RURXR0dHQxM2M7Mtmt4lu6SdgU8Dh/XGImIjsDHNL5K0Atif7ExjfG718cDq5mVrZma1tOLM4w+ApyPi7ctRkjokjUjz+wETgeciYg2wQdKRqZ3kDODWFuRsZmY5Vd6qewOwEDhAUo+ks9NbU9i2ofxjwBOSHgf+BTgnInob288F/hewHFiB77QyM2u5yi5bRcTUOvHP1YjdDNxcZ/lu4OCGJmdmZkPiJ8zNzKw0Fw8zMyvNxcPMzEpz8TAzs9JcPMzMrDQXDzMzK83Fw8zMSnPxMDOz0lw8zMysNBcPMzMrzcXDzMxKc/EwM7PSXDzMzKw0Fw8zMyvNxcPMzEpz8TAzs9JcPMzMrDQXDzMzK83Fw8zMSquseEi6RtI6SUtzsVmSXpS0OE0n5t6bKWm5pGckHZ+LHyZpSXrvCkmqKmczMyumyjOPa4HJNeLfjohJabodQNKBwBTgoLTO9ySNSMvPAaYBE9NUa5tmZtZElRWPiHgAeKng4icDN0bExohYCSwHjpA0FhgdEQsjIoAfAKdUkrCZmRXWijaP8yQ9kS5r7ZFi44AXcsv0pNi4NN83XpOkaZK6JXWvX7++0XmbmVnS7OIxB/gAMAlYA1ye4rXaMaKfeE0RMTciuiKiq6OjY4ipmplZPU0tHhGxNiI2R8RbwFXAEemtHmDv3KLjgdUpPr5G3MzMWqipxSO1YfQ6Fei9E2s+MEXSSEkTyBrGH46INcAGSUemu6zOAG5tZs5mZratnavasKQbgGOAMZJ6gEuAYyRNIrv0tAr4AkBELJN0E/AksAmYHhGb06bOJbtzazfgjjSZmVkLVVY8ImJqjfDV/Sw/G5hdI94NHNzA1MzMbIj8hLmZmZXm4mFmZqW5eJiZWWkuHmZmVpqLh5mZlTZg8ZD0GUmj0vxXJP1Y0qHVp2ZmZu2qyJnH30TEBklHAccD88i6GTEzsx1UkeLR+7DeScCciLgV2LW6lMzMrN0VKR4vSvo+cBpwu6SRBdczM7PtVJEicBpwJzA5Iv4D2BP4cpVJmZlZexuweETEG8A64KgU2gQ8W2VSZmbW3orcbXUJ8NfAzBTaBfinKpMyM7P2VuSy1anAp4DXASJiNTCqyqTMzKy9FSkev0njhweApHdVm5KZmbW7IsXjpnS31e6S/hy4m2wUQDMz20ENOJ5HRHxT0ieAV4EDgK9GxILKMzMzs7ZVaDCoVCxcMMzMDOineEjaQGrn6PsWEBExurKszMysrdVt84iIURExusY0qkjhkHSNpHWSluZi35D0tKQnJN0iafcU75T0a0mL03Rlbp3DJC2RtFzSFZI0xN/ZzMyGqFA3I5IOlXS+pL+UdEjBbV8LTO4TWwAcHBEfBv6dLc+OAKyIiElpOicXnwNMAyamqe82zcysyYo8JPhVsp503wuMAa6V9JWB1ouIB4CX+sTuiohN6eWDwPgB9j0WGB0RC9Ptwj8AThlo32ZmVq0iZx5TgcMj4pKIuAQ4EviTBuz7LOCO3OsJkh6TdL+ko1NsHNCTW6YnxWqSNE1St6Tu9evXNyBFMzOrpUjxWAW8I/d6JLBiKDuVdDFZH1nXpdAaYJ+IOAS4CLhe0miyxvm+ajXiZ29EzI2Irojo6ujoGEqKZmbWjyK36m4ElklaQPaH+xPAzyRdARAR55fZoaQzgU8Cx6ZLUUTExrQfImKRpBXA/mRnGvlLW+OB1WX2Z2ZmjVekeNySpl73DXZnkiaTdbL4e6m33t54B/BSRGyWtB9Zw/hzEfGSpA2SjgQeAs4AvjvY/ZuZWWMUecJ83mA2LOkG4BhgjKQe4BKyu6tGAgvSHbcPpjurPgZ8TdImspELz4mI3sb2c8nu3NqNrI0k305iZmYtMGDxkPRJ4OvAvmn5Qg8JRsTUGuGr6yx7M3Bznfe6gYMHytPMzJqnyGWr7wCfBpb0tlGYmdmOrcjdVi8AS104zMysV5Ezj/8G3C7pftIdUQAR8a3KsjIzs7ZWpHjMBl4je9Zj12rTMTOz4aBI8dgzIo6rPBMzMxs2irR53C3JxcPMzN5WpHhMB36aukx/NT2092rViZmZWfsq8pDgqGYkYmZmw0ehYWgl7UHWZcjbHSSmLtfNzGwHVOQJ8z8DLiDrlHAxWZfsC4HfrzQzMzNrW0XaPC4ADgeej4iPA4cAHizDzGwHVqR4vBkRbwJIGhkRTwMHVJuWmZm1syJtHj2Sdgf+law33JfxmBpmZju0IndbnZpmZ0m6F3gP8NNKs2qxzhm3tToFM7O2NuBlK0kfkDSy9yXQCbyzyqTMzKy9FWnzuBnYLOl3yMbjmABcX2lWZmbW1ooUj7ciYhNwKvCdiPgrYGy1aZmZWTsrUjx+K2kqcCbwkxTbpbqUzMys3RUpHp8H/jMwOyJWSpoA/FO1aZmZWTsbsHhExJMRcX5E3JBer4yISwdaT9I1ktZJWpqL7SlpgaRn0889cu/NlLRc0jOSjs/FD5O0JL13hSSV/zXNzKyRipx5DNa1wOQ+sRnAPRExEbgnvUbSgcAU4KC0zvckjUjrzAGmkfWtNbHGNs3MrMkqKx6p48SX+oRPBual+XnAKbn4jRGxMSJWAsuBIySNBUZHxMI0hvoPcuuYmVmL1C0ekn6Yfl7QwP3tFRFrANLP96X4OOCF3HI9KTYuzfeN18t5mqRuSd3r17v7LTOzqvR35nGYpH2BsyTtkdor3p4anEetdozoJ15TRMyNiK6I6Oro6GhYcmZmtrX+uie5kqwbkv2ARWz9hzxSvKy1ksZGxJp0SWpdivcAe+eWG0/Wf1ZPmu8bNzOzFqp75hERV0TEB4FrImK/iJiQmwZTOADmkz0vQvp5ay4+RdLIdCvwRODhdGlrg6Qj011WZ+TWMTOzFinSMeK5kj4CHJ1CD0TEEwOtJ+kG4BhgjKQe4BLgUuAmSWcDvwA+k/axTNJNwJPAJmB6RGxOmzqX7M6t3YA70tRW6nWkuOrSk5qciZlZcxQZSfB8sltlf5xC10maGxHf7W+9iJha561j6yw/G5hdI94NHDxQnmZm1jxFxvP4M+B3I+J1AEmXkQ1D22/xMDOz7VeR5zwEbM693kztu6DMzGwHUeTM4x+BhyTdkl6fQtY1u5mZ7aCKNJh/S9J9wFFkZxyfj4jHqk7MzMzaV5EzDyLiUeDRinMxM7NhosqOEc3MbDvl4mFmZqX1WzwkjZB0d7OSMTOz4aHf4pGe8n5D0nualI+ZmQ0DRRrM3wSWSFoAvN4bjIjzK8vKzMzaWpHicVuazMzMgGLPecyTtBuwT0Q804SczMyszQ14t5WkPwQWk43tgaRJkuZXnJeZmbWxIrfqzgKOAP4DICIWAxMqy8jMzNpekeKxKSJe6ROrOxSsmZlt/4o0mC+V9F+BEZImAucDP682LTMza2dFzjz+EjgI2AjcALwKXFhhTmZm1uaK3G31BnBxGgQqImJD9WmZmVk7K3K31eGSlgBPkD0s+Likw6pPzczM2lWRy1ZXA38REZ0R0QlMJxsgalAkHSBpcW56VdKFkmZJejEXPzG3zkxJyyU9I+n4we7bzMwao0iD+YaI+LfeFxHxM0mDvnSVHjScBFnHi8CLwC3A54FvR8Q388tLOhCYQtbu8n7gbkn7p363zMysBeoWD0mHptmHJX2frLE8gNOB+xq0/2OBFRHxvFR3WPSTgRsjYiOwUtJysudOFjYoBzMzK6m/M4/L+7y+JDffqOc8ppAVpV7nSToD6Aa+GBEvA+OAB3PL9KTYNiRNA6YB7LPPPg1K0czM+qpbPCLi41XuWNKuwKeAmSk0B/g6WWH6OlnxOots3PRt0qu1zYiYC8wF6Orq8oOMZmYVGbDNQ9LuwBlAZ375BnTJfgLwaESsTdtbm9vnVcBP0sseYO/ceuOB1UPct5mZDUGRu61uJyscS4BFuWmoppK7ZCVpbO69U4GlaX4+MEXSSEkTgInAww3Yv5mZDVKRu63eEREXNXKnkt4JfAL4Qi7895ImkV2SWtX7XkQsk3QT8CSwCZjuO63MzFqrSPH4oaQ/J7uMtLE3GBEvDXan6an19/aJ/Wk/y88GZg92f2Zm1lhFisdvgG8AF7OloTqA/apKyszM2luR4nER8DsR8cuqkzEzs+GhSIP5MuCNqhMxM7Pho8iZx2ZgsaR72brNY6i36pqZ2TBVpHj8a5rMzMyAYuN5zGtGImZmNnwUecJ8JTW6A4kI321lZraDKnLZqis3/w7gM8Ce1aRjZmbDwYB3W0XEr3LTixHxHeD3q0/NzMzaVZHLVofmXu5EdiYyqrKMzMys7RW5bJUf12MTWb9Tp1WSjZmZDQtF7raqdFwPMzMbfopcthoJ/BHbjufxterSMjOzdlbkstWtwCtkY3hsHGBZMzPbARQpHuMjYnLlmZiZ2bBRpGPEn0v6UOWZmJnZsFHkzOMo4HPpSfONgICIiA9XmpmZmbWtIsXjhMqzMDOzYaXIrbrPN3qnklYBG8i6e98UEV2S9gR+RHZX1yrgtIh4OS0/Ezg7LX9+RNzZ6JzMzKy4Im0eVfl4REyKiN6+s2YA90TEROCe9BpJBwJTgIOAycD3JI1oRcJmZpZpZfHo62Sgt/v3ecApufiNEbExIlYCy4Ejmp+emZn1alXxCOAuSYskTUuxvSJiDUD6+b4UHwe8kFu3J8W2IWmapG5J3evXr68odTMzK9JgXoWPRsRqSe8DFkh6up9lVSO2zfgiABExF5gL0NXVVXMZMzMbupaceUTE6vRzHXAL2WWotZLGAqSf69LiPcDeudXHA6ubl62ZmfXV9OIh6V2SRvXOA8cBS4H5wJlpsTPJukUhxadIGilpAjAReLi5WZuZWV4rLlvtBdwiqXf/10fETyU9Atwk6WzgF2QjFhIRyyTdBDxJ1iX89IjY3IK8S+uccVvN+KpLT2pyJmZmjdX04hERzwEfqRH/FXBsnXVmA7MrTs3MzApqp1t1zcxsmHDxMDOz0lw8zMysNBcPMzMrzcXDzMxKc/EwM7PSXDzMzKw0Fw8zMyvNxcPMzEpz8TAzs9JcPMzMrDQXDzMzK83Fw8zMSnPxMDOz0lw8zMysNBcPMzMrzcXDzMxKa8UwtDs8D09rZsOdzzzMzKy0phcPSXtLulfSU5KWSbogxWdJelHS4jSdmFtnpqTlkp6RdHyzczYzs6214rLVJuCLEfGopFHAIkkL0nvfjohv5heWdCAwBTgIeD9wt6T9I2JzU7M2M7O3Nf3MIyLWRMSjaX4D8BQwrp9VTgZujIiNEbESWA4cUX2mZmZWT0vbPCR1AocAD6XQeZKekHSNpD1SbBzwQm61HuoUG0nTJHVL6l6/fn1VaZuZ7fBaVjwkvRu4GbgwIl4F5gAfACYBa4DLexetsXrU2mZEzI2Irojo6ujoaHzSZmYGtKh4SNqFrHBcFxE/BoiItRGxOSLeAq5iy6WpHmDv3OrjgdXNzNfMzLbWirutBFwNPBUR38rFx+YWOxVYmubnA1MkjZQ0AZgIPNysfM3MbFutuNvqo8CfAkskLU6x/w5MlTSJ7JLUKuALABGxTNJNwJNkd2pN951WZmat1fTiERE/o3Y7xu39rDMbmF1ZUmZmVoqfMDczs9Lct1UbcZ9XZjZc+MzDzMxKc/EwM7PSXDzMzKw0Fw8zMyvNxcPMzEpz8TAzs9JcPMzMrDQ/5zEM1Hv+A/wMiJm1hs88zMysNBcPMzMrzcXDzMxKc5vHMOf+sMysFXzmYWZmpbl4mJlZaS4eZmZWmts8tlNuCzGzKvnMw8zMShs2xUPSZEnPSFouaUar8zEz25ENi8tWkkYA/xP4BNADPCJpfkQ82drMhp/+ujqppd5lLl8WM9uxDYviARwBLI+I5wAk3QicDLh4VKxssSm7fDO4oJk13nApHuOAF3Kve4Df7buQpGnAtPTyNUnPDHJ/Y4BfDnLdqjm3knTZ27NtmV/i3AbHuQ1evfz2LbLycCkeqhGLbQIRc4G5Q96Z1B0RXUPdThWc2+C1c37ObXCc2+ANNb/h0mDeA+ydez0eWN2iXMzMdnjDpXg8AkyUNEHSrsAUYH6LczIz22ENi8tWEbFJ0nnAncAI4JqIWFbhLod86atCzm3w2jk/5zY4zm3whpSfIrZpOjAzM+vXcLlsZWZmbcTFw8zMSnPxyGm3LlAk7S3pXklPSVom6YIUnyXpRUmL03Rii/JbJWlJyqE7xfaUtEDSs+nnHi3I64DcsVks6VVJF7bquEm6RtI6SUtzsbrHSdLM9B18RtLxLcrvG5KelvSEpFsk7Z7inZJ+nTuGV7Ygt7qfYzOPXZ3cfpTLa5WkxSne7ONW729H4753EeEpa/cZAawA9gN2BR4HDmxxTmOBQ9P8KODfgQOBWcCX2uCYrQLG9In9PTAjzc8ALmuDz/X/kT341JLjBnwMOBRYOtBxSp/v48BIYEL6To5oQX7HATun+cty+XXml2vRsav5OTb72NXKrc/7lwNfbdFxq/e3o2HfO595bPF2FygR8RugtwuUlomINRHxaJrfADxF9rR9OzsZmJfm5wGntC4VAI4FVkTE861KICIeAF7qE653nE4GboyIjRGxElhO9t1san4RcVdEbEovHyR7tqrp6hy7epp67PrLTZKA04Abqtp/f/r529Gw752Lxxa1ukBpmz/UkjqBQ4CHUui8dEnhmlZcGkoCuEvSotQ1DMBeEbEGsi8w8L4W5dZrClv/A26H4wb1j1M7fg/PAu7IvZ4g6TFJ90s6ukU51foc2+nYHQ2sjYhnc7GWHLc+fzsa9r1z8diiUBcorSDp3cDNwIUR8SowB/gAMAlYQ3Z63AofjYhDgROA6ZI+1qI8akoPlH4K+OcUapfj1p+2+h5KuhjYBFyXQmuAfSLiEOAi4HpJo5ucVr3PsZ2O3VS2/k9LS45bjb8ddRetEev32Ll4bNGWXaBI2oXsw78uIn4MEBFrI2JzRLwFXEXFlzXqiYjV6ec64JaUx1pJY1PuY4F1rcgtOQF4NCLWQvsct6TecWqb76GkM4FPAn8S6cJ4uqzxqzS/iOza+P7NzKufz7Etjp2knYFPAz/qjbXiuNX620EDv3cuHlu0XRco6brp1cBTEfGtXHxsbrFTgaV9121Cbu+SNKp3nqyBdSnZMTszLXYmcGuzc8vZ6n9/7XDccuodp/nAFEkjJU0AJgIPNzs5SZOBvwY+FRFv5OIdysbXQdJ+Kb/nmpxbvc+xLY4d8AfA0xHR0xto9nGr97eDRn7vmtX6Pxwm4ESyuxJWABe3QT5HkZ06PgEsTtOJwA+BJSk+Hxjbgtz2I7s743FgWe/xAt4L3AM8m37u2aJj907gV8B7crGWHDeyArYG+C3Z//DO7u84ARen7+AzwAktym852TXw3u/dlWnZP0qf9+PAo8AftiC3up9jM49drdxS/FrgnD7LNvu41fvb0bDvnbsnMTOz0nzZyszMSnPxMDOz0lw8zMysNBcPMzMrzcXDzMxKc/GwYU/SaxVsc1Kf3lpnSfrSELb3mdTD6b2NyXDQeaySNKaVOdj2wcXDrLZJZPfFN8rZwF9ExMcbuE2zlnHxsO2KpC9LeiR1mve3KdaZ/td/VRrb4C5Ju6X3Dk/LLlQ2hsXS1MPA14DT09gLp6fNHyjpPknPSTq/zv6nKhvjZKmky1Lsq2QPbV0p6Rt9lh8r6YG0n6W9HeZJmiOpO+X7t7nlV0n6HynfbkmHSrpT0gpJ56RljknbvEXSk5KulLTNv3VJn5X0cNr39yWNSNO1KZclkv5qiB+Jba+qfnrVk6eqJ+C19PM4YC5ZJ287AT8hG3Ohk6xzv0lpuZuAz6b5pcB/SfOXksZcAD4H/ENuH7OAn5ONdzCG7On1Xfrk8X7gF0AHsDPwf4BT0nv3AV01cv8iW57OHwGMSvN75mL3AR9Or1cB56b5b5M9QTwq7XNdih8DvEnWC8AIYAHwx7n1xwAfBP537+8AfA84AzgMWJDLb/dWf76e2nPymYdtT45L02NkXUD8J7I+egBWRsTiNL8I6FQ2Ot6oiPh5il8/wPZvi6yDu1+SdSi3V5/3Dwfui4j1kY2FcR1Z8erPI8DnJc0CPhTZ2AsAp0l6NP0uB5EN1tOrt8+1JcBDEbEhItYDb6bfCeDhyMam2UzWjcZRffZ7LFmheETZaHfHkhWb54D9JH039W/VX0+stgPbudUJmDWQgL+LiO9vFczGM9iYC20GdqN2N9T96buNvv9+ym6PiHggdWV/EvDDdFnr34AvAYdHxMuSrgXeUSOPt/rk9FYup779DvV9LWBeRMzsm5OkjwDHA9PJBjQ6q+zvZds/n3nY9uRO4Kw0hgGSxkmqOxhVRLwMbJB0ZApNyb29gexyUBkPAb8naUzqQXUqcH9/K0jal+xy01VkvaAeCowGXgdekbQXWdfyZR2ReojeCTgd+Fmf9+8B/rj3+Cgb23rfdCfWThFxM/A3KR+zbfjMw7YbEXGXpA8CC7MeqXkN+CzZWUI9ZwNXSXqdrG3hlRS/F5iRLun8XcH9r5E0M60r4PaIGKhL+mOAL0v6bcr3jIhYKekxsl5YnwP+b5H997GQrA3nQ8ADZOOt5HN9UtJXyEaC3ImsZ9jpwK+Bf8w1sG9zZmIGuFdd27FJendEvJbmZ5B1731Bi9MaEknHAF+KiE+2OBXbjvnMw3Z0J6WzhZ2B58nusjKzAfjMw8zMSnODuZmZlebiYWZmpbl4mJlZaS4eZmZWmouHmZmV9v8BUar8yZtjCPoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('문장 최대 길이 : %d' % max(len(l) for l in X_data))\n",
    "print('문장 평균 길이 : %f' % (sum(map(len, X_data))/len(X_data)))\n",
    "plt.hist([len(s) for s in X_data], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련 데이터, 학습 데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 및 학습 데이터 개수 비율 지정\n",
    "numtrain, numtest = num_dataset(0.8, X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 긴 길이로 맞추기\n",
    "X_data = pad_sequences(X_data, maxlen = maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import datasets\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "import pandas as pd\n",
    "import tensorflow.keras.metrics \n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    precision_v = precision(y_true, y_pred)\n",
    "    recall_v = recall(y_true, y_pred)\n",
    "    return 2*((precision_v*recall_v)/(precision_v+recall_v+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "325/325 [==============================] - 422s 1s/step - loss: 3.7203 - accuracy: 0.3233 - f1: 0.0543 - precision: 2334180.4998 - recall: 0.0274\n",
      "Epoch 2/5\n",
      "325/325 [==============================] - 426s 1s/step - loss: 1.2567 - accuracy: 0.3578 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 3/5\n",
      "325/325 [==============================] - 463s 1s/step - loss: 1.1471 - accuracy: 0.3444 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 4/5\n",
      "325/325 [==============================] - 695s 2s/step - loss: 1.1143 - accuracy: 0.3512 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 5/5\n",
      "325/325 [==============================] - 714s 2s/step - loss: 1.1193 - accuracy: 0.3621 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 1/5\n",
      "302/325 [==========================>...] - ETA: 50s - loss: 2.4180 - accuracy: 0.3444 - f1: 0.0125 - precision: 614551.3231 - recall: 0.0062"
     ]
    }
   ],
   "source": [
    "# size = 1000000\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(size , 64, input_length = maxlen)) #워드 임베딩\n",
    "# model.add(Dropout(0.5)) #과적합 방지를 위해 일부 Drop, 기본 50% 정도를 Drop하도록 설정함.\n",
    "# model.add(Conv1D(64, 3, padding='valid', activation='relu')) #hidden layer 추가\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "# model.add(Dropout(0.5)) #과적합 방지를 위해 일부 Drop, 기본 50% 정도를 Drop하도록 설정함.\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dropout(0.5)) #과적합 방지를 위해 일부 Drop, 기본 50% 정도를 Drop하도록 설정함.\n",
    "# num_of_class = 3 #클래스는 우선 4개로 분류함\n",
    "# model.add(Dense(num_of_class, activation='linear'))\n",
    "# ##model.add(Dense(num_of_class, activation='softmax')) #마지막 레이어는 softmax로 출력하게 함\n",
    "# model.summary()\n",
    "\n",
    "size = 1000000\n",
    "\n",
    "kfold = KFold(n_splits= 2, shuffle = True)\n",
    "\n",
    "for train, test in kfold.split(X_data, Y_data):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(size , 64, input_length = maxlen)) #워드 임베딩\n",
    "    model.add(Dropout(0.5)) #과적합 방지를 위해 일부 Drop, 기본 50% 정도를 Drop하도록 설정함.\n",
    "    model.add(Conv1D(64, 3, padding='valid', activation='relu')) #hidden layer 추가\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dropout(0.5)) #과적합 방지를 위해 일부 Drop, 기본 50% 정도를 Drop하도록 설정함.\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5)) #과적합 방지를 위해 일부 Drop, 기본 50% 정도를 Drop하도록 설정함.\n",
    "    num_of_class = 3 #클래스는 우선 4개로 분류함\n",
    "    model.add(Dense(num_of_class, activation='linear'))\n",
    "    ##model.add(Dense(num_of_class, activation='softmax')) #마지막 레이어는 softmax로 출력하게 함\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=[\"accuracy\",f1,precision, recall])\n",
    "    es = EarlyStopping(monitor='loss', mode='min' , min_delta=0)\n",
    "    check_point = ModelCheckpoint('best_model.h5', monitor='loss', mode='min', save_best_only=True)\n",
    "    \n",
    "    hist = model.fit(X_data, y_data, batch_size = 16, epochs=5, callbacks=[es, check_point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 194) for input KerasTensor(type_spec=TensorSpec(shape=(None, 194), dtype=tf.float32, name='embedding_3_input'), name='embedding_3_input', description=\"created by layer 'embedding_3_input'\"), but it was called on an input with incompatible shape (None, 1).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1478 predict_function  *\n        return step_function(self, iterator)\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1468 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1461 run_step  **\n        outputs = model.predict_step(data)\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1434 predict_step\n        return self(x, training=False)\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1012 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:375 call\n        return super(Sequential, self).call(inputs, training=training, mask=mask)\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:424 call\n        return self._run_internal_graph(\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:560 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1012 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py:248 call\n        outputs = self._convolution_op(inputs, self.kernel)\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:1013 convolution_v2\n        return convolution_internal(\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:1143 convolution_internal\n        return op(\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:605 new_func\n        return func(*args, **kwargs)\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:605 new_func\n        return func(*args, **kwargs)\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:1885 conv1d\n        result = gen_nn_ops.conv2d(\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py:968 conv2d\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:748 _apply_op_helper\n        op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:590 _create_op_internal\n        return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3528 _create_op_internal\n        ret = Operation(\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2015 __init__\n        self._c_op = _create_c_op(self._graph, node_def, inputs,\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1856 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Negative dimension size caused by subtracting 3 from 1 for '{{node sequential_3/conv1d_3/conv1d}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](sequential_3/conv1d_3/conv1d/ExpandDims, sequential_3/conv1d_3/conv1d/ExpandDims_1)' with input shapes: [?,1,1,64], [1,3,64,64].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-9e64b31294a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mX_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1627\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1628\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1629\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1630\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1631\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    860\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2939\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m-> 2941\u001b[1;33m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[0;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3355\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3356\u001b[0m               call_context_key in self._function_cache.missed):\n\u001b[1;32m-> 3357\u001b[1;33m             return self._define_function_with_shape_relaxation(\n\u001b[0m\u001b[0;32m   3358\u001b[0m                 args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0;32m   3359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[1;34m(self, args, kwargs, flat_args, filtered_flat_args, cache_key_context)\u001b[0m\n\u001b[0;32m   3277\u001b[0m           expand_composites=True)\n\u001b[0;32m   3278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3279\u001b[1;33m     graph_function = self._create_graph_function(\n\u001b[0m\u001b[0;32m   3280\u001b[0m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0;32m   3281\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3194\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3195\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3196\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3197\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3198\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 990\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    975\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 977\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    978\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1478 predict_function  *\n        return step_function(self, iterator)\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1468 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1461 run_step  **\n        outputs = model.predict_step(data)\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1434 predict_step\n        return self(x, training=False)\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1012 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:375 call\n        return super(Sequential, self).call(inputs, training=training, mask=mask)\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:424 call\n        return self._run_internal_graph(\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:560 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1012 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py:248 call\n        outputs = self._convolution_op(inputs, self.kernel)\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:1013 convolution_v2\n        return convolution_internal(\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:1143 convolution_internal\n        return op(\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:605 new_func\n        return func(*args, **kwargs)\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:605 new_func\n        return func(*args, **kwargs)\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:1885 conv1d\n        result = gen_nn_ops.conv2d(\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py:968 conv2d\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:748 _apply_op_helper\n        op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:590 _create_op_internal\n        return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3528 _create_op_internal\n        ret = Operation(\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2015 __init__\n        self._c_op = _create_c_op(self._graph, node_def, inputs,\n    C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1856 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Negative dimension size caused by subtracting 3 from 1 for '{{node sequential_3/conv1d_3/conv1d}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](sequential_3/conv1d_3/conv1d/ExpandDims, sequential_3/conv1d_3/conv1d/ExpandDims_1)' with input shapes: [?,1,1,64], [1,3,64,64].\n"
     ]
    }
   ],
   "source": [
    "X_result = model.predict(test, batch_size = 16)\n",
    "X_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "678",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3079\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3080\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 678",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-46fc843d80e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mfor_open_max_0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    852\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 853\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    854\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    855\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m    959\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m         \u001b[1;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 961\u001b[1;33m         \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    962\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3080\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3082\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3084\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 678"
     ]
    }
   ],
   "source": [
    "class_result = []\n",
    "for a,b,c in X_result:\n",
    "    numbers = [a,b,c]\n",
    "    class_result.append(numbers.index(max(numbers)))\n",
    "    \n",
    "tf_result = (y_data == class_result)\n",
    "\n",
    "for_open_max_0 = [] \n",
    "for_open_max_1 = []\n",
    "for_open_max_2 = []\n",
    "\n",
    "for i in range(len(class_result)):\n",
    "    if(tf_result[i] == True):\n",
    "        if(class_result[i] == 0):\n",
    "            for_open_max_0.append(X_result[i])\n",
    "        if(class_result[i] == 1):\n",
    "            for_open_max_1.append(X_result[i])\n",
    "        if(class_result[i] == 2):\n",
    "            for_open_max_2.append(X_result[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평균 Logit Vector 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_vector(for_open_max, class_result):\n",
    "    a1=0\n",
    "    a2=0\n",
    "    a3=0\n",
    "    for i in for_open_max:\n",
    "        a1 += i[0]\n",
    "        a2 += i[1]\n",
    "        a3 += i[2]\n",
    "    length = len(class_result)\n",
    "    average = [a1/length, a2/length, a3/length]\n",
    "    return average\n",
    "\n",
    "average_0 = average_vector(for_open_max_0, class_result) #평균 Logit Vector - Class 0\n",
    "average_1 = average_vector(for_open_max_1, class_result) #평균 Logit Vector - Class 1\n",
    "average_2 = average_vector(for_open_max_2, class_result) #평균 Logit Vector - Class 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(for_open_max, average):\n",
    "    dist = []\n",
    "    for i in for_open_max:\n",
    "        m = i - average\n",
    "        distance = (m[0]**2) + (m[1]**2) + (m[2]**2)\n",
    "        dist.append(distance)\n",
    "    return dist\n",
    "\n",
    "dist0 = distance(for_open_max_0, average_0)\n",
    "dist1 = distance(for_open_max_1, average_1)\n",
    "dist2 = distance(for_open_max_2, average_2)\n",
    "\n",
    "dist0.sort(reverse =True)\n",
    "dist1.sort(reverse =True)\n",
    "dist2.sort(reverse =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance 상위 20개 추출\n",
    "maxdist0=[]\n",
    "maxdist1=[]\n",
    "maxdist2=[]\n",
    "for i in range(0,20):\n",
    "    maxdist0.append(dist0[i])\n",
    "for j in range(0,20):\n",
    "    maxdist1.append(dist1[j])\n",
    "for k in range(0,20):\n",
    "    maxdist2.append(dist2[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#극단 분포 도구\n",
    "import scipy.stats as s\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#def weib(x,n,a):\n",
    "#    return (a / n) * (x / n)**(a - 1) * np.exp(-(x / n)**a)\n",
    "def weib(x,n,a):\n",
    "    return 1-np.exp[-(x / n)**a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 라벨 Distance 추출 Weibull 극단 분포 CDF\n",
    "(loc, scale) = s.exponweib.fit_loc_scale(maxdist0, 1, 1)\n",
    "print(loc, scale)\n",
    "\n",
    "plt.plot(maxdist0, s.exponweib.cdf(maxdist0, *s.exponweib.fit(maxdist0, 1, 1, scale=2, loc=0)))\n",
    "_ = plt.hist(maxdist0, bins=np.linspace(0, 16, len(maxdist0)), alpha=0.5);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 라벨 Distance 추출 Weibull 극단 분포 CDF\n",
    "(loc, scale) = s.exponweib.fit_loc_scale(maxdist1, 1, 1)\n",
    "print(loc, scale)\n",
    "\n",
    "plt.plot(maxdist1, s.exponweib.cdf(maxdist1, *s.exponweib.fit(maxdist1, 1, 1, scale=2, loc=0)))\n",
    "_ = plt.hist(maxdist1, bins=np.linspace(0, 16, len(maxdist1)), alpha=0.5);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2 라벨 Distance 추출 Weibull 극단 분포 CDF\n",
    "(loc, scale) = s.exponweib.fit_loc_scale(maxdist2, 1, 1)\n",
    "print(loc, scale)\n",
    "\n",
    "plt.plot(maxdist2, s.exponweib.cdf(maxdist2, *s.exponweib.fit(maxdist2, 1, 1, scale=2, loc=0)))\n",
    "_ = plt.hist(maxdist2, bins=np.linspace(0, 16, len(maxdist2)), alpha=0.5);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculCDF(dist):\n",
    "#     CDF = []\n",
    "#     for i in dist:\n",
    "#         CDF.append(s.exponweib.cdf(i, *s.exponweib.fit(i, 1, 1, scale=2, loc=0)))\n",
    "#     return CDF\n",
    "# CDF0 = calculCDF(dist0)\n",
    "# CDF1 = calculCDF(dist1)\n",
    "# CDF2 = calculCDF(dist2)\n",
    "\n",
    "# updated_logit = []\n",
    "# i = 0\n",
    "# for a,b,c in X_result:\n",
    "#     logit0 = a-(a*CDF0[i])\n",
    "#     logit1 = b-(b*CDF1[i])\n",
    "#     logit2 = c-(c*CDF2[i])\n",
    "#     unkn_logit = CDF0[i]*a + CDF1[i]*b + CDF2[i]*c    # unknown class의 logit vector\n",
    "#     updated_logit.append([unkn_logit, logit0, logit1, logit2])\n",
    "#     i += 1\n",
    "# # update 된 logit veoctor로 softmax layer 통과\n",
    "# print(updated_logit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Layer 통과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(num_of_class + 1, activation = 'softmax'))\n",
    "model2.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics= [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model2.fit(X_result, y_test, batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model2.predict(X_result, batch_size = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 학습 과정 표시\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "\n",
    "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
    "\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(model.predict(X_test, batch_size=16),columns=['0', '1', '2'])\n",
    "result[\"Test Label\"] = y_test\n",
    "result[\"Classification Result\"] = class_result\n",
    "result[\"Final Result\"] = (y_test == class_result)\n",
    "\n",
    "result.to_csv(\"test_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"테스트 정확도: %.4f\" % (model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##모델을 .json 파일 형식으로 save하여 저장\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file : \n",
    "    json_file.write(model_json)\n",
    "    \n",
    "model.save_weights(\"model_weight.h5\")\n",
    "model.save('full_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##모델 조정이 끝나면, Model을 사용 하여 OpenMax에 필요한 자료를 선정하여 OpenMax로 구현할 예정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
