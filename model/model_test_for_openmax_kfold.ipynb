{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_data = pd.read_csv(\"hate_speech_binary_dataset.csv\", delimiter=\",\") # 혐오 문장\n",
    "genderbias_data = pd.read_csv('genderbias.csv', sep=',')  # 여성 비하 문장\n",
    "ilbe_data = pd.read_csv('badword.csv',encoding='CP949') # 일베 문장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문장 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 혐오 문장 처리\n",
    "hate_data.columns = ['comment', 'label'] # 컬럼 명 변경\n",
    "hate_data = hate_data.astype({'comment': 'str'})\n",
    "hate_data = hate_data[hate_data['label']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여성 비하 문장 처리\n",
    "del genderbias_data['bias']    #해당 데이터셋의 필요없는 열 제거\n",
    "del genderbias_data['hate']    #해당 데이터셋의 필요없는 열 제거\n",
    "genderbias_data['contain_gender_bias'] = genderbias_data['contain_gender_bias'].replace([False, True],[0,1])  # 구분하기 쉽게 기존의 표기를 0,1로 변경\n",
    "# genderbias_data = genderbias_data[['contain_gender_bias', 'comments']]    #구분하기 쉽게 열의 순서를 변경\n",
    "\n",
    "genderbias_data.columns = ['comment', 'label'] # 컬럼 명 변경\n",
    "genderbias_data = genderbias_data[genderbias_data['label']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일베 문장 처리\n",
    "ilbe_data = ilbe_data[['v2', 'v1']]    #구분하기 쉽게 열의 순서를 변경\n",
    "ilbe_data.columns = ['comment', 'label'] # 컬럼 명 변경\n",
    "ilbe_data = ilbe_data[ilbe_data['label']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hate_data : 100000\n",
      "genderbias_data : 1232\n",
      "ilbe_data : 2044\n"
     ]
    }
   ],
   "source": [
    "# 데이터 개수\n",
    "print(\"hate_data : %d\" % len(hate_data))\n",
    "print(\"genderbias_data : %d\" % len(genderbias_data))\n",
    "print(\"ilbe_data : %d\" % len(ilbe_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라벨링 및 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 렌덤하게 문장 추출 후 라벨링 거치는 함수\n",
    "def random_labeling(data, classified_data, label_num, str_num):  # label_num : 라벨링 시킬 값, str_num 추출할 문장 개수\n",
    "    random_data = classified_data.sample(n=str_num) # str_num 개의 행(문장) 랜덤 추출\n",
    "    random_data.loc[random_data.label == 1, 'label'] = label_num # label_num으로 값 변경\n",
    "    data = data.append(random_data)  # data 에 랜덤 추출된 데이터 추가\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>str_Declaration</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           comment  label\n",
       "0  str_Declaration     -1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_declaration = {\n",
    "    'comment' : [\"str_Declaration\"],\n",
    "    'label':[-1]\n",
    "}\n",
    "\n",
    "\n",
    "data = pd.DataFrame(data_declaration)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 개수를 모두 2000 개로 지정 각 \n",
    "# 라벨은 혐오:0, 여성비하: 1, 일베: 2로 지정됨\n",
    "data = random_labeling(data, hate_data, 0, 2000)\n",
    "data = random_labeling(data, genderbias_data, 1, 1200)\n",
    "data = random_labeling(data, ilbe_data, 2, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 첫번째 행 제거\n",
    "data = data.drop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 랜덤하게 섞기 \n",
    "data = data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>지들이 무식한건 지들 잘못이지 왜 남들한테 그 잘못을 씌우냐고 시발아</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>사람 위에 군림하는 정권들... 인권이 뭔지 모르는 인간들, 10년이 훨 지난 일이...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>아 정말 tv 이쁘고 귀여운애들만 나왔으면 좋겠다. 쉰것들 쫌!!!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>귀엽고 발랄한 영화♪</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>미개한 문화</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>걸작 코메디</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>지극히 아련하다.. 리메이크됐으면!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>이쁘니깐 괜찮아</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>그러니까 딸 그만 쳐라</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>좌파는 그냥 북한 보내서 김정은 충성하며 '인민'으로 사는게 딱 맞는 수준이다.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  label\n",
       "0             지들이 무식한건 지들 잘못이지 왜 남들한테 그 잘못을 씌우냐고 시발아      2\n",
       "1  사람 위에 군림하는 정권들... 인권이 뭔지 모르는 인간들, 10년이 훨 지난 일이...      0\n",
       "2              아 정말 tv 이쁘고 귀여운애들만 나왔으면 좋겠다. 쉰것들 쫌!!!      2\n",
       "3                                        귀엽고 발랄한 영화♪      0\n",
       "4                                            미개한 문화       2\n",
       "5                                             걸작 코메디      0\n",
       "6                                지극히 아련하다.. 리메이크됐으면!      0\n",
       "7                                           이쁘니깐 괜찮아      1\n",
       "8                                       그러니까 딸 그만 쳐라      2\n",
       "9       좌파는 그냥 북한 보내서 김정은 충성하며 '인민'으로 사는게 딱 맞는 수준이다.      2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 중복 및 null 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_overlap(data):\n",
    "    exist_overlap = len(data)  # 데이터 전체 개수\n",
    "    no_overlap = data['comment'].nunique()  # 중복 제거된 개수\n",
    "    if exist_overlap != no_overlap:\n",
    "        data.drop_duplicates(subset=['comment'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_overlap(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# null 값 확인\n",
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x_data, tokenizer):\n",
    "    tokenizer.fit_on_texts(x_data) # 데이터의 각 행별로 토큰화 수행\n",
    "    return tokenizer.texts_to_sequences(x_data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = data['comment']\n",
    "y_data = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "sequences = tokenize(x_data, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 희귀단어 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_rareword(tokenizer, threshold):\n",
    "    word_to_index = tokenizer.word_index \n",
    "    total_cnt = len(word_to_index) # 단어의 수\n",
    "    rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "    total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "    rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "    # 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "    for key, value in tokenizer.word_counts.items():\n",
    "        total_freq = total_freq + value\n",
    "\n",
    "        # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "        if(value < threshold):\n",
    "            rare_cnt = rare_cnt + 1\n",
    "            rare_freq = rare_freq + value\n",
    "\n",
    "    print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "    print(\"단어 집합(vocabulary)에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "    print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "등장 빈도가 1번 이하인 희귀 단어의 수: 23538\n",
      "단어 집합(vocabulary)에서 희귀 단어의 비율: 83.11734171404358\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 48.6664185584915\n"
     ]
    }
   ],
   "source": [
    "# 희귀 단어 확인\n",
    "detect_rareword(tokenizer, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련 데이터, 테스트 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 긴 길이\n",
    "def max_length(X_data):\n",
    "    return max(len(l) for l in X_data)\n",
    "\n",
    "# 훈련 학습 데이터 개수\n",
    "def num_dataset(raio, X_data):\n",
    "    num_train = int(len(X_data)*0.8)\n",
    "    num_test = int(len(X_data) - num_train)\n",
    "    return num_train, num_test\n",
    "\n",
    "# 훈련 및 학습 데이터 분리\n",
    "def config_dataset(num_train, data):\n",
    "    X_test = data[num_train:] #X_data 데이터 중에서 뒤의 개의 데이터만 저장\n",
    "    y_test = np.array(y_data[num_train:]) #y_data 데이터 중에서 뒤의 개의 데이터만 저장\n",
    "    X_train = data[:num_train] #X_data 데이터 중에서 앞의 n_of_train개의 데이터만 저장\n",
    "    y_train = np.array(y_data[:num_train]) #y_data 데이터 중에서 앞의 n_of_train개의 데이터만 저장\n",
    "    \n",
    "    return X_test, y_test, X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 길이 계산\n",
    "X_data = sequences\n",
    "maxlen = max_length(X_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문장 길이 분포 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 최대 길이 : 194\n",
      "문장 평균 길이 : 9.319075\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('문장 최대 길이 : %d' % max(len(l) for l in X_data))\n",
    "print('문장 평균 길이 : %f' % (sum(map(len, X_data))/len(X_data)))\n",
    "plt.hist([len(s) for s in X_data], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련 데이터, 학습 데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 및 학습 데이터 개수 비율 지정\n",
    "numtrain, numtest = num_dataset(0.8, X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 긴 길이로 맞추기\n",
    "data = pad_sequences(X_data, maxlen = maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test, X_train, y_train = config_dataset(numtrain, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import datasets\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "import pandas as pd\n",
    "import tensorflow.keras.metrics \n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 교차 검증 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    precision_v = precision(y_true, y_pred)\n",
    "    recall_v = recall(y_true, y_pred)\n",
    "    return 2*((precision_v*recall_v)/(precision_v+recall_v+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-b87367c434a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mkfold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkfold\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#워드 임베딩\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Y_data' is not defined"
     ]
    }
   ],
   "source": [
    "# size = 1000000\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(size , 64, input_length = maxlen)) #워드 임베딩\n",
    "# model.add(Dropout(0.5)) #과적합 방지를 위해 일부 Drop, 기본 50% 정도를 Drop하도록 설정함.\n",
    "# model.add(Conv1D(64, 3, padding='valid', activation='relu')) #hidden layer 추가\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "# model.add(Dropout(0.5)) #과적합 방지를 위해 일부 Drop, 기본 50% 정도를 Drop하도록 설정함.\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dropout(0.5)) #과적합 방지를 위해 일부 Drop, 기본 50% 정도를 Drop하도록 설정함.\n",
    "# num_of_class = 3 #클래스는 우선 4개로 분류함\n",
    "# model.add(Dense(num_of_class, activation='linear'))\n",
    "# ##model.add(Dense(num_of_class, activation='softmax')) #마지막 레이어는 softmax로 출력하게 함\n",
    "# model.summary()\n",
    "\n",
    "size = 1000000\n",
    "\n",
    "kfold = KFold(n_splits= 5, shuffle = True)\n",
    "\n",
    "for train, test in kfold.split(X_data, Y_data):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(size , 64, input_length = maxlen)) #워드 임베딩\n",
    "    model.add(Dropout(0.5)) #과적합 방지를 위해 일부 Drop, 기본 50% 정도를 Drop하도록 설정함.\n",
    "    model.add(Conv1D(64, 3, padding='valid', activation='relu')) #hidden layer 추가\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dropout(0.5)) #과적합 방지를 위해 일부 Drop, 기본 50% 정도를 Drop하도록 설정함.\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5)) #과적합 방지를 위해 일부 Drop, 기본 50% 정도를 Drop하도록 설정함.\n",
    "    num_of_class = 3 #클래스는 우선 4개로 분류함\n",
    "    model.add(Dense(num_of_class, activation='linear'))\n",
    "    ##model.add(Dense(num_of_class, activation='softmax')) #마지막 레이어는 softmax로 출력하게 함\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=[\"accuracy\",f1,precision, recall])\n",
    "    es = EarlyStopping(monitor='loss', mode='min' , min_delta=0)\n",
    "    check_point = ModelCheckpoint('best_model.h5', monitor='loss', mode='min', save_best_only=True)\n",
    "    \n",
    "    hist = model.fit(X_data, y_data, batch_size = 16, epochs=10, callbacks=[es, check_point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_result = model.predict(X_test, batch_size = 16)\n",
    "X_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_result = []\n",
    "for a,b,c in X_result:\n",
    "    numbers = [a,b,c]\n",
    "    class_result.append(numbers.index(max(numbers)))\n",
    "    \n",
    "tf_result = (y_test == class_result)\n",
    "\n",
    "for_open_max_0 = []; \n",
    "for_open_max_1 = [];\n",
    "for_open_max_2 = [];\n",
    "\n",
    "for i in range(len(class_result)):\n",
    "    if(tf_result[i] == True):\n",
    "        if(class_result[i] == 0):\n",
    "            for_open_max_0.append(X_result[i])\n",
    "        if(class_result[i] == 1):\n",
    "            for_open_max_1.append(X_result[i])\n",
    "        if(class_result[i] == 2):\n",
    "            for_open_max_2.append(X_result[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평균 Logit Vector 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_vector(for_open_max, class_result):\n",
    "    a1=0\n",
    "    a2=0\n",
    "    a3=0\n",
    "    for i in for_open_max:\n",
    "        a1 += i[0]\n",
    "        a2 += i[1]\n",
    "        a3 += i[2]\n",
    "    length = len(class_result)\n",
    "    average = [a1/length, a2/length, a3/length]\n",
    "    return average\n",
    "\n",
    "average_0 = average_vector(for_open_max_0, class_result) #평균 Logit Vector - Class 0\n",
    "average_1 = average_vector(for_open_max_1, class_result) #평균 Logit Vector - Class 1\n",
    "average_2 = average_vector(for_open_max_2, class_result) #평균 Logit Vector - Class 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(for_open_max, average):\n",
    "    dist = []\n",
    "    for i in for_open_max:\n",
    "        m = i - average\n",
    "        distance = (m[0]**2) + (m[1]**2) + (m[2]**2)\n",
    "        dist.append(distance)\n",
    "    return dist\n",
    "\n",
    "dist0 = distance(for_open_max_0, average_0)\n",
    "dist1 = distance(for_open_max_1, average_1)\n",
    "dist2 = distance(for_open_max_2, average_2)\n",
    "\n",
    "dist0.sort(reverse =True)\n",
    "dist1.sort(reverse =True)\n",
    "dist2.sort(reverse =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance 상위 20개 추출\n",
    "maxdist0=[]\n",
    "maxdist1=[]\n",
    "maxdist2=[]\n",
    "for i in range(0,20):\n",
    "    maxdist0.append(dist0[i])\n",
    "for j in range(0,20):\n",
    "    maxdist1.append(dist1[j])\n",
    "for k in range(0,20):\n",
    "    maxdist2.append(dist2[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#극단 분포 도구\n",
    "import scipy.stats as s\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#def weib(x,n,a):\n",
    "#    return (a / n) * (x / n)**(a - 1) * np.exp(-(x / n)**a)\n",
    "def weib(x,n,a):\n",
    "    return 1-np.exp[-(x / n)**a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 라벨 Distance 추출 Weibull 극단 분포 CDF\n",
    "(loc, scale) = s.exponweib.fit_loc_scale(maxdist0, 1, 1)\n",
    "print(loc, scale)\n",
    "\n",
    "plt.plot(maxdist0, s.exponweib.cdf(maxdist0, *s.exponweib.fit(maxdist0, 1, 1, scale=2, loc=0)))\n",
    "_ = plt.hist(maxdist0, bins=np.linspace(0, 16, 33), alpha=0.5);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 라벨 Distance 추출 Weibull 극단 분포 CDF\n",
    "(loc, scale) = s.exponweib.fit_loc_scale(maxdist1, 1, 1)\n",
    "print(loc, scale)\n",
    "\n",
    "plt.plot(maxdist1, s.exponweib.cdf(maxdist1, *s.exponweib.fit(maxdist1, 1, 1, scale=2, loc=0)))\n",
    "_ = plt.hist(maxdist1, bins=np.linspace(0, 16, 33), alpha=0.5);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 라벨 Distance 추출 Weibull 극단 분포 CDF\n",
    "(loc, scale) = s.exponweib.fit_loc_scale(maxdist2, 1, 1)\n",
    "print(loc, scale)\n",
    "\n",
    "plt.plot(maxdist2, s.exponweib.cdf(maxdist2, *s.exponweib.fit(maxdist2, 1, 1, scale=2, loc=0)))\n",
    "_ = plt.hist(maxdist2, bins=np.linspace(0, 16, 33), alpha=0.5);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Layer 통과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(num_of_class + 1, activation = 'softmax'))\n",
    "model2.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics= [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.fit(X_result, y_test, batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.predict(X_result, batch_size = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 학습 과정 표시\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "\n",
    "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
    "\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(model.predict(X_test, batch_size=16),columns=['0', '1', '2'])\n",
    "result[\"Test Label\"] = y_test\n",
    "result[\"Classification Result\"] = class_result\n",
    "result[\"Final Result\"] = (y_test == class_result)\n",
    "\n",
    "result.to_csv(\"test_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"테스트 정확도: %.4f\" % (model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##모델을 .json 파일 형식으로 save하여 저장\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file : \n",
    "    json_file.write(model_json)\n",
    "    \n",
    "model.save_weights(\"model_weight.h5\")\n",
    "model.save('full_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##모델 조정이 끝나면, Model을 사용 하여 OpenMax에 필요한 자료를 선정하여 OpenMax로 구현할 예정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
